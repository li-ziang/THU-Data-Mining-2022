# Data-Mining

这是2022年数据挖掘大作业的报告，作业内容为天猫复购概率预测挑战赛项目。项目成员为李伟楷，李子昂，夏箫（以字母排序）。

## 总览

我们实现了三种不同的算法用于商品复购概率，分别为

* 随机森林+XGBoost

TODO:

* MLP

TODO:

* GraphSAGE

我们将用户和商家作为异质节点，以商家和用户之间的交易关系作为边，构造异构图，并使用异构图上的神经网络`GraphSAGE`进行学习。



这三个模型分别代表了研究者处理数据挖掘问题的三个阶段。第一个阶段是使用传统的机器学习方法，使用好的特征加上适当的分类器，以及特定的集成学习方法就能得到好的结果。第二阶段是使用深度学习方法，我们使用MLP这一典型的深度学习方法，使用与第一种方法类似的特征工程，并且比较了二者的差异。第三阶段是使用图表示学习方法，这种方法的特征工程与前两种有大的区别，我们特征工程的区别将进行了比较，并给出了相对合适的特征。

## 0. 运行

```python
pip install -r requirments.txt
```

TODO:等xx merge完之后把requirments.txt生成一下

## 1.数据分析与预处理

DataFrame中有一些数据是缺失的，如果直接将csv读成DataFrame则会出现nan，需要进行判断并将其赋值成合适的值。

异构图：图中包含了424170个独立的用户节点，以及1994个商家节点。在`user_info_format1.csv`中，以用户和商家之间的交易信息为边（同一组用户和商家之间只能有一条边），则一共有14052685条边。如果仅以`train_format1.csv`与`test_format1.csv`中的交易数据建边，则有522341条边，其中train集中有260864条边，test集中有261477条边，他们全部都在前面提到的14052685条边中，这也意味着他们的feature都是可以根据`user_info_format1.csv`中的信息建立的。

## 4.GraphSAGE

### 4.1代码

模型主要由两部分文件构成，`dataset.py`中的`TaobaoDataset`是一个[PyG](https://www.pyg.org/)风格的图数据集，在这个文件中我进行了原始数据的处理，并将其处理成一个可以直接用torch加载的`.pt`文件。原始文件需要手动放在`./data/taobao/raw`下，生成的文件将会储存在`./data/taobao/processed`下，具体如下:

```shell
(base) ziang@ubuntu:/data/ziang/data-mining$ ls data/taobao/raw/
test_format1.csv   user_info_format1.csv
train_format1.csv  user_log_format1.csv
(base) ziang@ubuntu:/data/ziang/data-mining$ ls data/taobao/processed/
data.pt  pre_filter.pt  pre_transform.pt
```

在第一次运行`TaobaoDataset`时，会从`raw`中读取文件并运行`process`函数，最后生成`data.pt`文件。之后如果再次运行`TaobaoDataset`，则会从`data.pt`中直接读取处理好的文件。需要运行训练和评估时，直接运行`heterogeneous.py`即可。

### 4.2特征工程

我尝试直接从`user_log_format1.csv`建立图。

在异构图学习中，节点与边都可以给特征，我设计的基础特征如下：

* 用户节点（冒号后为对应的维度）

年龄：[1]，性别：[1]。

张量大小：[424170,2]

* 商家节点

使用一个商家是否卖过某个category的商品，也就是判断是否包含了某个`cat_id`是否出现在了当前商家的销售信息中，如果出现了则对应的值设置为1，否则为0。特征的维度为[1658]，也就是独立的`cat_id`的个数。

张量大小：[1994, 1672]

* 边

我直接使用了`user_log_format1.csv`中的数据，举个例子：

```python
user_log.head()
Out[158]: 
  user_id  item_id  cat_id  seller_id  brand_id  time_stamp  action_type
  0   328862   323294     833       2882    2661.0         829            0
```

可以看到`user_log`的存在形式是`user_id->seller_id`，边没有feature，但是任然需要存储空间。

张量大小：[2, 5551143]

**Mark**：从输入上看，仍然还有`user_log`中的交易信息没有存放到图中，特征工程的重点应该放在如何将这五个信息加入到图中。在这之后，如果有一个合理的图学习框架，则可以有效的学到每个节点相应的表示。

### 4.3 基于显存的特征工程优化

* 对于图结构数据，比较重要的一个因素就是显存问题，这是因为图数据的节点和边的feature是一次性输入的，没有办法像cv或nlp中的数据分batch进行输入，所以必须严格控制显存。可以看到，三个组feature占用的显存大小大致为`M(边)=10*M(商家)=100*M(用户)`，这是因为边的总数较多，而且用户节点的feature维度较少造成的。考虑到数量上`N(商家)<<N(用户)<<N(边)`，**合理的做法应该是优先减少边的个数，其次是加大商家的feature，最后才是加大用户的feature。**在后面的优化中，我也会尊崇这个法则。

* 其次，在实验上，使用1.2中的所有边需要33G的显存才能将实验顺利运行完成（这里我使用的是一张48G的Titan 8000），远远超出了一般显卡的内存空间。而如果我们在构造异构图的时候只使用`train`和`test`中存在的边（一共有522341条，见第1部分），需要的显存将会大幅减小至10G左右，是一般显卡可以承受的。两种feature构造方式对结果的影响见4.4。

* 对于商家节点，因为其个数较少，并且往往一个商家节点会与很多用户节点相连，（$\frac{d(商家)}{d(用户)}=213$其中d表示相应节点的度），所以在商家的节点中提供更多的信息其实是显存友好的。从图结构上考虑，商家节点有更多的可能性将其feature传递到周围的用户节点，这样使用少的显存储存和传递了更多的信息。4.2中的商家节点只使用了`cat_id`作为feature，其实可以增加的feature是`brand_id`。

## 分工

我们采用了每个人主要负责一个模型，其他同学辅助的形式。我们在特征工程上彼此分享了对自己模型有显著贡献的特征。`随机森林+XGBoost`的主要负责人是夏箫，`MLP`的主要负责人是李伟楷，`GraphSAGE`的主要负责人是李子昂。此外，李子昂和李伟楷负责了数据的统计分析和预处理。总体上三个人贡献相同。
